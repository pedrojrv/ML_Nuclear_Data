import os
from pathlib import Path
from joblib import dump, load
import logging
import glob
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split

logging.basicConfig(level=logging.INFO)

import nucml.general_utilities as gen_utils # pylint: disable=import-error
import nucml.processing as nuc_proc # pylint: disable=import-error

################################################################################
# DEFINING NECESSARY PATHS TO FIND DATASETS 
################################################################################
# ml_nuclear_path = "/Users/pedrovicentevaldez/Desktop/ML_Nuclear_Data/"


# TO DO: MAKE PATH ABSOLUTE FOR WINDOWS AND MAC
ame_dir_path = "C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\AME\\CSV_Files\\"
def load_ame(directory=ame_dir_path, natural=False, nan=False):
    """Loads the Atomic Mass Evaluation 2016 data generated by NucML using the parsing utilities.

    Args:
        directory (str): string representing the path to the directory where the AME files are stored.
        natural (bool): if True, the AME data will be loaded along with rows representing natural data.
        nan (bool): If True, the original AME data will be loaded. If False, then the linearly imputed data is loaded.

    Returns:
        ame (DataFrame): a pandas dataframe cantaining the AME data.

    """
    if natural:
        if nan:
            ame_file_path = os.path.join(directory, "AME_Natural_Properties_w_NaN.csv")
        else:
            ame_file_path = os.path.join(directory, "AME_Natural_Properties_no_NaN.csv")
    else:
        if nan:
            ame_file_path = os.path.join(directory, "AME_all_merged.csv")
        else:
            ame_file_path = os.path.join(directory, "AME_all_merged_no_NaN.csv")
    if os.path.exists(ame_file_path):
        logging.info("AME: Reading and loading Atomic Mass Evaluation files from: \n {}".format(ame_file_path))
        ame = pd.read_csv(ame_file_path)
        ame[["N", "Z", "A"]] = ame[["N", "Z", "A"]].astype(int)
        return ame
    else:
        logging.error("AME: Requested file does not exists. Check that it exists on the given directory path. \
            If it does not exists then it probably hasn't been generated.")
        return None

def load_ame_mass16(directory=ame_dir_path):
    """Loads the Atomic Mass Evaluation 2016 Mass 16 data generated by NucML using the parsing utilities.

    Args:
        directory (str): string representing the path to the directory where the Mass 16 AME file is stored.

    Returns:
        ame (DataFrame): a pandas dataframe cantaining the Mass 16 AME data.

    """

    ame_file_path = os.path.join(directory, "AME_mass16.csv")
    if os.path.exists(ame_file_path):
        logging.info("AME MASS16: Reading and loading the Atomic Mass Evaluation Mass 16 file from: \n {}".format(ame_file_path))
        ame = pd.read_csv(ame_file_path)
        return ame
    else:
        logging.error("AME MASS16: Requested file does not exists. Check that it exists on the given directory path. \
            If it does not exists then it probably hasn't been generated.")
        return None

def load_ame_rct1(directory=ame_dir_path):
    """Loads the Atomic Mass Evaluation 2016 RCT1 data generated by NucML using the parsing utilities.

    Args:
        directory (str): string representing the path to the directory where the RCT1 AME file is stored.

    Returns:
        ame (DataFrame): a pandas dataframe cantaining the Mass 16 AME data.

    """

    ame_file_path = os.path.join(directory, "AME_rct1.csv")
    if os.path.exists(ame_file_path):
        logging.info("AME RCT1: Reading and loading the Atomic Mass Evaluation RCT1 file from: \n {}".format(ame_file_path))
        ame = pd.read_csv(ame_file_path)
        return ame
    else:
        logging.error("AME RCT1: Requested file does not exists. Check that it exists on the given directory path. \
            If it does not exists then it probably hasn't been generated.")
        return None

def load_ame_rct2(directory=ame_dir_path):
    """Loads the Atomic Mass Evaluation 2016 RCT2 data generated by NucML using the parsing utilities.

    Args:
        directory (str): string representing the path to the directory where the RCT2 AME file is stored.

    Returns:
        ame (DataFrame): a pandas dataframe cantaining the RCT2 AME data.

    """

    ame_file_path = os.path.join(directory, "AME_rct2.csv")
    if os.path.exists(ame_file_path):
        logging.info("AME RCT2: Reading and loading the Atomic Mass Evaluation RCT1 file from: \n {}".format(ame_file_path))
        ame = pd.read_csv(ame_file_path)
        return ame
    else:
        logging.error("AME RCT2: Requested file does not exists. Check that it exists on the given directory path. \
            If it does not exists then it probably hasn't been generated.")
        return None

def load_endf(ELAAA, MT, mode="neutrons", mev_to_ev=True, mb_to_b=True, log=False, drop_u=True):
    """Reads Evaluated Nuclear Data File for a specific element and reaction channel. It is important
    to inspect the returned data since it queries an external database which extracted data from ENDF 
    using a particular script. It has been found that some particular reactions are not included.

    Args:
        elementELAAA (str): element to query. Must be in ELAAA format (i.e. U233, Cl35). An error will be 
            raised if the arguments are not formated correctly.
        MT (str): reaction channel to query. Must be in MT### format (i.e. MT018, MT101)
        mev_to_ev (bool): if True, it converts energy from meV to eV.
        mb_to_b (bool): if True, it converts the cross sections from mb to b.
        log (bool): if True, it applies the log10 to both the Energy and Cross Section features.
        drop_u (bool): sometimes, endf files contain uncertainty values. If True, these features are removed.
        mode (str): experimental feature. Only "neutrons" is allowed for now.

    Returns:
        endf (DataFrame): pandas DataFrame containing the ENDF datapoints.
        None: if file does not exist. 
    """
    endf_path = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\Evaluated_Data\\'
    path = os.path.join(endf_path, mode + "/" + ELAAA + "/endfb8.0/tables/xs/n-" + ELAAA + "-" + MT + ".endfb8.0")
    file = Path(path)
    if file.is_file():
        logging.info("ENDF: Extracting data from {}".format(path))
        # uranium is in MeV, we need eV
        endf = pd.read_csv(path, skiprows=5, header=None, names=["Energy", "Data", "dDataLow", "dDataUpp"], delim_whitespace=True)
        if mev_to_ev:
            logging.info("ENDF: Converting MeV to eV...")
            endf["Energy"] = endf["Energy"]*1E6
        if mb_to_b:
            logging.info("ENDF: Converting mb to b...")
            endf["Data"] = endf["Data"]*0.001
        if log:
            endf["Energy"] = np.log10(endf["Energy"])
            endf["Data"] = np.log10(endf["Data"])
            endf["dDataLow"] = np.log10(endf["dDataLow"])
            endf["dDataUpp"] = np.log10(endf["dDataUpp"])
        if drop_u:
            if "dData" in list(endf.columns):
                endf = endf.drop(columns=["dDataLow"])
            if "dData2" in list(endf.columns):
                endf = endf.drop(columns=["dDataUpp"])
            if "dDataLow" in list(endf.columns):
                endf = endf.drop(columns=["dDataLow"])
            if "dDataUpp" in list(endf.columns):
                endf = endf.drop(columns=["dDataUpp"])
        logging.info("ENDF: Finished. ENDF data contains {} datapoints.".format(endf.shape[0]))
        return endf
    else:
        logging.info("ENDF: File does not exists. Check path.")
        return None

def load_tendl(ELAAA, MT, mode="neutrons", mev_to_ev=False, mb_to_b=False, log=False, drop_u=False):
    """Reads Evaluated Nuclear Data File for a specific element and reaction channel. It is important
    to inspect the returned data since it queries an external database which extracted data from ENDF 
    using a particular script. It has been found that some particular reactions are not included.

    Args:
        elementELAAA (str): element to query. Must be in ELAAA format (i.e. U233, Cl35). An error will be 
            raised if the arguments are not formated correctly.
        MT (str): reaction channel to query. Must be in MT### format (i.e. MT018, MT101)
        mev_to_ev (bool): if True, it converts energy from meV to eV.
        mb_to_b (bool): if True, it converts the cross sections from mb to b.
        log (bool): if True, it applies the log10 to both the Energy and Cross Section features.
        drop_u (bool): sometimes, endf files contain uncertainty values. If True, these features are removed.
        mode (str): experimental feature. Only "neutrons" is allowed for now.
    Returns:
        DataFrame: (Energy, Cross Section) Pandas DataFrame.
        None: if file does not exist. 

    """
    # path = os.path.abspath("../ENDF/ENDF_" + mode + "/" + ELAAA + "/endfb8.0/tables/xs/n-" + ELAAA + "-" + MT + ".endfb8.0")
    endf_path = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\Evaluated_Data\\'
    path = os.path.join(endf_path, mode + "/" + ELAAA + "/tendl.2019/tables/xs/n-" + ELAAA + "-" + MT + ".tendl.2019")
    file = Path(path)
    if file.is_file():
        logging.info("ENDF: Extracting data from {}".format(path))
        # uranium is in MeV, we need eV
        endf = pd.read_csv(path, skiprows=5, header=None, names=["Energy", "Data", "dDataLow", "dDataUpp"], delim_whitespace=True)
        if mev_to_ev:
            logging.info("ENDF: Converting MeV to eV...")
            endf["Energy"] = endf["Energy"]*1E6
        if mb_to_b:
            logging.info("ENDF: Converting mb to b...")
            endf["Data"] = endf["Data"]*0.001
        if log:
            endf["Energy"] = np.log10(endf["Energy"])
            endf["Data"] = np.log10(endf["Data"])
            endf["dDataLow"] = np.log10(endf["dDataLow"])
            endf["dDataUpp"] = np.log10(endf["dDataUpp"])
        if drop_u:
            if "dData" in list(endf.columns):
                endf = endf.drop(columns=["dDataLow"])
            if "dData2" in list(endf.columns):
                endf = endf.drop(columns=["dDataUpp"])
        logging.info("ENDF: Finished. ENDF data contains {} datapoints.".format(endf.shape[0]))
        return endf
    else:
        logging.info("ENDF: File does not exists. Check path.")
        return None

def load_jeff(ELAAA, MT, mode="neutrons", mev_to_ev=False, mb_to_b=False, log=False, drop_u=False):
    """Reads Evaluated Nuclear Data File for a specific element and reaction channel. It is important
    to inspect the returned data since it queries an external database which extracted data from ENDF 
    using a particular script. It has been found that some particular reactions are not included.

    Args:
        elementELAAA (str): element to query. Must be in ELAAA format (i.e. U233, Cl35). An error will be 
            raised if the arguments are not formated correctly.
        MT (str): reaction channel to query. Must be in MT### format (i.e. MT018, MT101)
        mev_to_ev (bool): if True, it converts energy from meV to eV.
        mb_to_b (bool): if True, it converts the cross sections from mb to b.
        log (bool): if True, it applies the log10 to both the Energy and Cross Section features.
        drop_u (bool): sometimes, endf files contain uncertainty values. If True, these features are removed.
        mode (str): experimental feature. Only "neutrons" is allowed for now.
    Returns:
        DataFrame: (Energy, Cross Section) Pandas DataFrame.
        None: if file does not exist. 

    """
    # path = os.path.abspath("../ENDF/ENDF_" + mode + "/" + ELAAA + "/endfb8.0/tables/xs/n-" + ELAAA + "-" + MT + ".endfb8.0")
    endf_path = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\Evaluated_Data\\'
    path = os.path.join(endf_path, mode + "/" + ELAAA + "/jeff3.3/tables/xs/n-" + ELAAA + "-" + MT + ".jeff3.3")
    file = Path(path)
    if file.is_file():
        logging.info("ENDF: Extracting data from {}".format(path))
        # uranium is in MeV, we need eV
        endf = pd.read_csv(path, skiprows=5, header=None, names=["Energy", "Data", "dDataLow", "dDataUpp"], delim_whitespace=True)
        if mev_to_ev:
            logging.info("ENDF: Converting MeV to eV...")
            endf["Energy"] = endf["Energy"]*1E6
        if mb_to_b:
            logging.info("ENDF: Converting mb to b...")
            endf["Data"] = endf["Data"]*0.001
        if log:
            endf["Energy"] = np.log10(endf["Energy"])
            endf["Data"] = np.log10(endf["Data"])
            endf["dDataLow"] = np.log10(endf["dDataLow"])
            endf["dDataUpp"] = np.log10(endf["dDataUpp"])
        if drop_u:
            if "dData" in list(endf.columns):
                endf = endf.drop(columns=["dDataLow"])
            if "dData2" in list(endf.columns):
                endf = endf.drop(columns=["dDataUpp"])
        logging.info("ENDF: Finished. ENDF data contains {} datapoints.".format(endf.shape[0]))
        return endf
    else:
        logging.info("ENDF: File does not exists. Check path.")
        return None

def load_jendl(ELAAA, MT, mode="neutrons", mev_to_ev=False, mb_to_b=False, log=False, drop_u=False):
    """Reads Evaluated Nuclear Data File for a specific element and reaction channel. It is important
    to inspect the returned data since it queries an external database which extracted data from ENDF 
    using a particular script. It has been found that some particular reactions are not included.

    Args:
        elementELAAA (str): element to query. Must be in ELAAA format (i.e. U233, Cl35). An error will be 
            raised if the arguments are not formated correctly.
        MT (str): reaction channel to query. Must be in MT### format (i.e. MT018, MT101)
        mev_to_ev (bool): if True, it converts energy from meV to eV.
        mb_to_b (bool): if True, it converts the cross sections from mb to b.
        log (bool): if True, it applies the log10 to both the Energy and Cross Section features.
        drop_u (bool): sometimes, endf files contain uncertainty values. If True, these features are removed.
        mode (str): experimental feature. Only "neutrons" is allowed for now.
    Returns:
        DataFrame: (Energy, Cross Section) Pandas DataFrame.
        None: if file does not exist. 

    """
    # path = os.path.abspath("../ENDF/ENDF_" + mode + "/" + ELAAA + "/endfb8.0/tables/xs/n-" + ELAAA + "-" + MT + ".endfb8.0")
    endf_path = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\Evaluated_Data\\'
    path = os.path.join(endf_path, mode + "/" + ELAAA + "/jendl4.0/tables/xs/n-" + ELAAA + "-" + MT + ".jendl4.0")
    file = Path(path)
    if file.is_file():
        logging.info("ENDF: Extracting data from {}".format(path))
        # uranium is in MeV, we need eV
        endf = pd.read_csv(path, skiprows=5, header=None, names=["Energy", "Data", "dDataLow", "dDataUpp"], delim_whitespace=True)
        if mev_to_ev:
            logging.info("ENDF: Converting MeV to eV...")
            endf["Energy"] = endf["Energy"]*1E6
        if mb_to_b:
            logging.info("ENDF: Converting mb to b...")
            endf["Data"] = endf["Data"]*0.001
        if log:
            endf["Energy"] = np.log10(endf["Energy"])
            endf["Data"] = np.log10(endf["Data"])
            endf["dDataLow"] = np.log10(endf["dDataLow"])
            endf["dDataUpp"] = np.log10(endf["dDataUpp"])
        if drop_u:
            if "dData" in list(endf.columns):
                endf = endf.drop(columns=["dDataLow"])
            if "dData2" in list(endf.columns):
                endf = endf.drop(columns=["dDataUpp"])
        logging.info("ENDF: Finished. ENDF data contains {} datapoints.".format(endf.shape[0]))
        return endf
    else:
        logging.info("ENDF: File does not exists. Check path.")
        return None

def load_ensdf(cutoff=False, log_sqrt=False, log=False, append_ame=False, basic=-1, num=False, frac=0.3, scaling_type="pt", scaler_dir=None):
    """Loads the Evalauted Nuclear Structure Data File generated using NucML. This allows the user to load
    the raw file or preprocessed the dataset for ML applications. See options below.

    The basic feature allows you to load only some basic features if needed. The AME dataset contains
    many features including q-reactions and separation energies. Some of these may not be needed. The
    basic argument allows to quickly remove extra features. 
        basic = 0: "Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", "Atomic_Mass_Micro"
        basic = 1: "Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", 
            "Atomic_Mass_Micro", 'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'S(2n)', 'S(n)', 'S(p)'
        Any other number will default to loading the entire set of features.

    Args:
        cutoff (bool, optional): If True, the RIPL cutoff ENSDF file is loaded. Defaults to False.
        log (bool, optional): If True, the log10 is applied to the Level Number feature. It also applies 
            the square root to the Level Energy feature. Defaults to False.
        append_ame (bool, optional): if True, it appends the AME database. Defaults to False.
        basic (int, optional): This allows to retrieve only basic features. 
            Only meaningful when append_ame is True. Defaults to -1.
        num (bool, optional): [description]. Defaults to False.
        frac (float, optional): [description]. Defaults to 0.3.
        scaling_type (str, optional): [description]. Defaults to "pt".
        scaler_dir ([type], optional): [description]. Defaults to None.

    Returns:
        DataFrame: if num=True, the function returns 6 variables. 
    """    
    if cutoff:
        datapath = "../../ENSDF/CSV_Files/ensdf_cutoff.csv"
    else:
        datapath = "../../ENSDF/CSV_Files/ensdf.csv"
    if os.path.exists(datapath):
        logging.info("Reading data from {}".format(datapath))
        df = pd.read_csv(datapath)
        df = df[~(df["Parity"] == 1620)] # THIS ELIMINATES A STRANGE DATAPOINT
        df["Level_Number"] = df["Level_Number"].astype(int)
        df[["Spin", "Parity", "Target_Element_w_A"]] = df[["Spin", "Parity", "Target_Element_w_A"]].astype('category')
        if log_sqrt:
            df["Level_Energy"] = np.sqrt(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if log:
            logging.info("Dropping Ground State...")
            df = df[(df["Level_Energy"] != 0)]
            df["Level_Energy"] = np.log10(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if append_ame:
            ame = load_ame(natural=False, nan=False)
            ame = ame.rename(columns={"Element_w_A": "Target_Element_w_A", "Z": "Protons", "N": "Neutrons", "A": "Mass_Number"})
            df = pd.merge(df, ame, on='Target_Element_w_A')
        if basic == 0:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", "Atomic_Mass_Micro"]
            df = df[basic_cols]
        elif basic == 1:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", "Atomic_Mass_Micro",
                        'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'S(2n)', 'S(n)', 'S(p)']
            df = df[basic_cols] 
        if num:
            logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")
            if basic == 0 or basic == 1:
                cat_cols = ["Parity"]
            else:
                columns_drop = ["Target_Element_w_A", "EL", "O"]
                cat_cols = ["Parity"]
                df = df.drop(columns=columns_drop)
            # We need to keep track of columns to normalize excluding categorical data.
            norm_columns = len(df.columns) - len(cat_cols) - 1
            df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)
            df = df.fillna(value=0)
            logging.info("Splitting dataset into training and testing...")
            x_train, x_test, y_train, y_test = train_test_split(df.drop(["Level_Energy"], axis=1), df["Level_Energy"], test_size=frac)
            logging.info("Normalizing dataset...")
            to_scale = list(x_train.columns)[:norm_columns]
            if log_sqrt or log:
                to_scale.remove("Level_Number")
            if scaler_dir is not None:
                logging.info("Using previously saved scaler.")
                scaler = load(open(scaler_dir, 'rb'))
            else:
                logging.info("Fitting new scaler.")
                if scaling_type == "pt":
                    scaler = preprocessing.PowerTransformer().fit(x_train[to_scale])
                elif scaling_type == "std":
                    scaler = preprocessing.StandardScaler().fit(x_train[to_scale])
                elif scaling_type == "minmax":
                    scaler = preprocessing.MinMaxScaler().fit(x_train[to_scale])
            x_train[to_scale] = scaler.transform(x_train[to_scale])
            x_test[to_scale] = scaler.transform(x_test[to_scale])
            logging.info("Finished. Resulting dataset has shape {}, Training and Testing dataset shapes are {} and {} respesctively.".format(df.shape, x_train.shape, x_test.shape))
            return df, x_train, x_test, y_train, y_test, to_scale, scaler
        else:
            logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
            return df
    else:
        return logging.error("CSV file does not exists. Check path.")

def load_ripl_parameters():
    """Loads the RIPL level cut-off parameters file.

    Returns:
        DataFrame: pandas DataFrame 
    """    
    ripl_params = pd.read_csv("../ENSDF/CSV_Files/ripl_cut_off_energies.csv")
    return ripl_params

def load_ensdf_ml(log_sqrt=False, log=False, append_ame=False, basic=-1, num=False, frac=0.3, scaling_type="std", scaler_dir=None):
    path = '../../ENSDF/CSV_Files/Level_Density' # use your path
    all_files = glob.glob(path + "/*.csv")

    if len(all_files) != 0:

        li = []

        for filename in all_files:
            df = pd.read_csv(filename, index_col=None, header=0)
            li.append(df)

        df = pd.concat(li, axis=0, ignore_index=True)

        df["Level_Number"] = df["Level_Number"].astype(int)
        df[["Target_Element_w_A"]] = df[["Target_Element_w_A"]].astype('category')
        if log_sqrt:
            df["Level_Energy"] = np.sqrt(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if log:
            logging.info("Dropping Ground State...")
            df = df[(df["Level_Energy"] != 0)]
            df["Level_Energy"] = np.log10(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if append_ame:
            ame = load_ame(natural=False, nan=False)
            ame = ame.rename(columns={"Element_w_A": "Target_Element_w_A", "Z": "Protons", "N": "Neutrons", "A": "Mass_Number"})
            df = pd.merge(df, ame, on='Target_Element_w_A')
        if basic == 0:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Atomic_Mass_Micro"]
            df = df[basic_cols]
        elif basic == 1:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Atomic_Mass_Micro",
                        'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'S(2n)', 'S(n)', 'S(p)']
            df = df[basic_cols] 
        if num:
            logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")

            # We need to keep track of columns to normalize excluding categorical data.
            df = df.fillna(value=0)
            logging.info("Splitting dataset into training and testing...")
            x_train, x_test, y_train, y_test = train_test_split(df.drop(["Level_Energy"], axis=1), df["Level_Energy"], test_size=frac)
            logging.info("Normalizing dataset...")
            to_scale = list(x_train.columns)
            if log_sqrt or log:
                to_scale.remove("Level_Number")
            if scaler_dir is not None:
                logging.info("Using previously saved scaler.")
                scaler = load(open(scaler_dir, 'rb'))
            else:
                logging.info("Fitting new scaler.")
                if scaling_type == "pt":
                    scaler = preprocessing.PowerTransformer().fit(x_train[to_scale])
                elif scaling_type == "std":
                    scaler = preprocessing.StandardScaler().fit(x_train[to_scale])
                elif scaling_type == "minmax":
                    scaler = preprocessing.MinMaxScaler().fit(x_train[to_scale])
            x_train[to_scale] = scaler.transform(x_train[to_scale])
            x_test[to_scale] = scaler.transform(x_test[to_scale])
            logging.info("Finished. Resulting dataset has shape {}, Training and Testing dataset shapes are {} and {} respesctively.".format(df.shape, x_train.shape, x_test.shape))
            return df, x_train, x_test, y_train, y_test, to_scale, scaler
        else:
            logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
            return df
    else:
        return logging.error("CSV file does not exists. Check path.")


supported_modes = ["neutrons", "protons", "alphas", "deuterons", "gammas", "helions", "all"]
supported_mt_coding = ["one_hot", "particle_coded"]
def load_exfor(log=False, low_en=False, basic=-1, num=False, frac=0.1, mode="neutrons", scaling_type="standard", 
    scaler_dir=None, filters=False, max_en=2.0E7, mt_coding="one_hot"):

    if mode not in supported_modes:
        return logging.error("Specified MODE not supported. Supporte modes include: {}".format(' '.join([str(v) for v in supported_modes])))
    if mt_coding not in supported_mt_coding:
        return logging.error("Specified mt_coding not supported. Supported codings include: {}".format(' '.join([str(v) for v in supported_mt_coding])))

    logging.info(" MODE: {}".format(mode))
    logging.info(" LOW ENERGY: {}".format(low_en))
    logging.info(" LOG: {}".format(log))
    logging.info(" BASIC: {}".format(basic))
    logging.info(" SCALER: {}".format(scaling_type.upper()))

    if mode == "all":
        neutrons_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "neutrons" + '\\EXFOR_' + "neutrons" + '_MF3_AME_no_RawNaN.csv'
        protons_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "protons" + '\\EXFOR_' + "protons" + '_MF3_AME_no_RawNaN.csv'
        alphas_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "alphas" + '\\EXFOR_' + "alphas" + '_MF3_AME_no_RawNaN.csv'
        deuterons_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "deuterons" + '\\EXFOR_' + "deuterons" + '_MF3_AME_no_RawNaN.csv'
        gammas_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "gammas" + '\\EXFOR_' + "gammas" + '_MF3_AME_no_RawNaN.csv'
        helions_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "helions" + '\\EXFOR_' + "helions" + '_MF3_AME_no_RawNaN.csv'
        all_datapaths = [neutrons_datapath, protons_datapath, alphas_datapath, deuterons_datapath, gammas_datapath, helions_datapath]
        if gen_utils.check_if_files_exist(all_datapaths):
            df = pd.read_csv(neutrons_datapath, dtype=dtype_exfor).dropna()
            protons = pd.read_csv(protons_datapath, dtype=dtype_exfor).dropna()
            alphas = pd.read_csv(alphas_datapath, dtype=dtype_exfor).dropna()
            deuterons = pd.read_csv(deuterons_datapath, dtype=dtype_exfor).dropna()
            gammas = pd.read_csv(gammas_datapath, dtype=dtype_exfor).dropna()
            helions = pd.read_csv(helions_datapath, dtype=dtype_exfor).dropna()
            df = df.append([protons, alphas, deuterons, gammas, helions])
        else:
            return logging.error("One ore more files are missing. Check directories.")
    else:
        datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + mode + '\\EXFOR_' + mode + '_MF3_AME_no_RawNaN.csv'
        if os.path.exists(datapath):
            logging.info("Reading data from {}".format(datapath))
            df = pd.read_csv(datapath, dtype=dtype_exfor).dropna()
        else:
            return logging.error("CSV file does not exists. Check given path: {}".format(datapath))
        
    if filters:
        df = df[~((df.Reaction_Notation.str.contains("WTR")) | (df.Title.str.contains("DERIV")) | (df.Energy == 0) | (df.Data == 0))]
        df = df[(df.MT != "203") & (df.MT != "1003") & (df.MT != "1108") & (df.MT != "2103")]
    if low_en:
        df = df[df.Energy < max_en]
    if log:
        if (df[df.Energy == 0].shape[0] != 0) or (df[df.Data == 0].shape[0] != 0):
            logging.error("Cannot take log. Either Energy or Data contain zeros. Ignoring log.")
        else:
            df["Energy"] = np.log10(df["Energy"])
            df["Data"] = np.log10(df["Data"])
    if basic == 0:
        basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag", "N", "A", "Element_Flag"]
        df = df[basic_cols]
    elif basic == 1:
        # 'S(2p)' THIS IS AN ISSUE WITH YEO TRANSFORMER FOR SOME REASON SO WATCH OUT!
        basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag",
                    "N", "A", "Element_Flag", 'Nucleus_Radius', 'Neutron_Nucleus_Radius_Ratio', 
                    'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'Atomic_Mass_Micro', 'S(2n)', 
                    'S(n)', 'S(p)', 'S(2p)']
        df = df[basic_cols]  

    logging.info("Data read into dataframe with shape: {}".format(df.shape))
    if num:
        logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")
        if basic == 0 or basic == 1:
            columns_drop = ["dData", "dEnergy"]
            cat_cols = ["MT", "Center_of_Mass_Flag", "Element_Flag"]
        else:
            columns_drop = ["Projectile", "EXFOR_Status", "Short_Reference", 'EXFOR_Accession_Number', 'EXFOR_SubAccession_Number', 
                            'EXFOR_Pointer', "Reaction_Notation", "Title", "Year", "Author", "Institute", "Date", "Reference",
                            'Dataset_Number', 'EXFOR_Entry', 'Reference_Code', 'Isotope', 'Element', "dData", "dEnergy",
                            'Projectile_Z', 'Projectile_A', 'Projectile_N']
            cat_cols = ["Target_Metastable_State", "MT", "Product_Metastable_State", "Center_of_Mass_Flag", "I78", "Element_Flag", "O"]
        
            
        df.drop(columns=columns_drop, inplace=True)
        
        if mt_coding == "particle_coded":
            cat_cols.remove("MT")
            mt_codes_df = pd.read_csv('C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\mt_codes.csv').drop(columns=["MT_Tag", "MT_Reaction_Notation"])
            mt_codes_df["MT"] = mt_codes_df["MT"].astype(str)
            # We need to keep track of columns to normalize excluding categorical data.
            norm_columns = len(df.columns) - len(cat_cols) - 2
            df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)
            df = pd.merge(df, mt_codes_df, on='MT').drop(columns=["MT"])
        elif mt_coding == "one_hot":
            # We need to keep track of columns to normalize excluding categorical data.
            norm_columns = len(df.columns) - len(cat_cols) - 1
            df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)


        logging.info("Splitting dataset into training and testing...")
        x_train, x_test, y_train, y_test = train_test_split(df.drop(["Data"], axis=1), df["Data"], test_size=frac)
        
        logging.info("Normalizing dataset...")
        to_scale = list(x_train.columns)[:norm_columns]
        to_scale.remove("Energy")
        scaler = nuc_proc.normalize_features(x_train, to_scale, scaling_type, scaler_dir)
        x_train[to_scale] = scaler.transform(x_train[to_scale])
        x_test[to_scale] = scaler.transform(x_test[to_scale])
        return df, x_train, x_test, y_train, y_test, to_scale, scaler
    else:
        logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
        return df




# exfor_elements = gen_utils.load_obj(os.path.join(os.path.dirname(__file__), 'objects/exfor_elements.pkl'))
# element_info = gen_utils.load_obj(os.path.join(os.path.dirname(__file__), 'objects/element_basic_dict.pkl'))
dtype_exfor = gen_utils.load_obj(os.path.join(os.path.dirname(__file__), 'objects/EXFOR_AME_dtypes.pkl'))

# IMPLEMENT ADDITION OF STABLE STATES
# columns_ensdf = ["Element_w_A", "N1", "Elv[MeV]", "spin", "parity", "state_half_life", "Ng", "J", "unc", "spins", "nd", 
#                  "m", "percent", "mode", "other", "other1", "other2", "other3", "other4"]
# ensdf_final = pd.read_csv(resulting_files_dir + "ensdf_stable_state_formatted.csv", names=columns_ensdf, sep=";")
# ensdf_final["spin"] = ensdf_final["spin"].replace(to_replace=-1.0, value=3.5) 
# ensdf_final["parity"] = ensdf_final["parity"].replace(to_replace=0, value=1.0)
# ensdf_final["Element_w_A"] = ensdf_final["Element_w_A"].apply(lambda x: x.strip())
# ensdf_final = ensdf_final[["Element_w_A", "spin", "parity"]]
# df2 = pd.merge(df, ensdf_final, on='Element_w_A')

# ADD RIPLE LEVEL PARAMETERES LOADING
# cut_off_cols = ["Z", "A", "Element", "Temperature_MeV", "Temperature_U", "Black_Shift", 
#                 "Black_Shift_U", "N_Lev_ENSDF", "N_Max_Lev_Complete", "Min_Lev_Complete", 
#                 "Num_Lev_Unique_Spin", "E_Max_N_Max", "E_Num_Lev_U_Spin", "Other", "Other2", 
#                 "Flag", "Nox", "Other3", "Other4", "Spin_Cutoff"]
# cut_off = pd.read_csv("./ENSDF/Resulting_Files/cut_off_ensdf_energies.csv", names=cut_off_cols, sep=";")

# cut_off.tail()

# df.dtypes.apply(lambda x: x.name).to_dict()


# # ADD CAPABILITY TO ADD ENSDF AND AME SEPARATEDLEY
# supported_modes = ["neutrons", "protons", "alphas", "deuterons", "gammas", "helions", "all"]
# def load_exfor(log=False, low_en=True, basic=-1, num=False, frac=0.1, mode="neutrons", scaling_type="pt", scaler_dir=None, filters=True):
#     """[summary]

#     Args:
#         log (bool, optional): [description]. Defaults to False.
#         low_en (bool, optional): [description]. Defaults to True.
#         basic (int, optional): [description]. Defaults to -1.
#         num (bool, optional): [description]. Defaults to False.
#         frac (float, optional): [description]. Defaults to 0.1.
#         mode (str, optional): [description]. Defaults to "neutrons".
#         scaling_type (str, optional): [description]. Defaults to "pt".
#         scaler_dir ([type], optional): [description]. Defaults to None.

#     Returns:
#         [type]: [description]
#     """
    
#     ######## SETTING UP VARIABLES FOR DATA EXTRACTION ##########
#     if mode not in supported_modes:
#         return logging.error("Specified MODE not supported. Supporte modes include: {}".format(' '.join([str(v) for v in supported_modes])))

#     logging.info(" MODE: {}".format(mode))
#     logging.info(" LOW ENERGY: {}".format(low_en))
#     logging.info(" LOG: {}".format(log))
#     logging.info(" BASIC: {}".format(basic))
#     logging.info(" SCALER: {}".format(scaling_type.upper()))

#     datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + mode + '\\EXFOR_' + mode + '_MF3_AME_no_RawNaN.csv'
#     if os.path.exists(datapath):
#         logging.info("Reading data from {}".format(datapath))
#         df = pd.read_csv(datapath, dtype=dtype_exfor).dropna()

#         if filters:
#             df = df[~((df.Reaction_Notation.str.contains("WTR")) | (df.Title.str.contains("DERIV")) | (df.Energy == 0) | (df.Data == 0))]

#         if low_en:
#             df = df[df.Energy < 2.0E7]

#         if log:
#             if (df[df.Energy == 0].shape[0] != 0) or (df[df.Data == 0].shape[0] != 0):
#                 logging.error("Cannot take log. Either Energy or Data contain zeros. Ignoring log.")
#             else:
#                 df["Energy"] = np.log10(df["Energy"])
#                 df["Data"] = np.log10(df["Data"])

#         if basic == 0:
#             basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag", "N", "A", "Element_Flag"]
#             df = df[basic_cols]
#         elif basic == 1:
#             # 'S(2p)' THIS IS AN ISSUE WITH YEO TRANSFORMER FOR SOME REASON
#             basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag",
#                         "N", "A", "Element_Flag", 'Nucleus_Radius', 'Neutron_Nucleus_Radius_Ratio', 
#                         'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'Atomic_Mass_Micro', 'S(2n)', 
#                         'S(n)', 'S(p)']
#             df = df[basic_cols]  

#         logging.info("Data read into dataframe with shape: {}".format(df.shape))
#         if num:
#             logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")
#             if basic == 0 or basic == 1:
#                 columns_drop = ["dData", "dEnergy"]
#                 cat_cols = ["MT", "Center_of_Mass_Flag", "Element_Flag"]
#             else:
#                 columns_drop = ["Projectile", "EXFOR_Status", "Short_Reference", 'EXFOR_Accession_Number', 'EXFOR_SubAccession_Number', 
#                                 'EXFOR_Pointer', "Reaction_Notation", "Title", "Year", "Author", "Institute", "Date", "Reference",
#                                 'Dataset_Number', 'EXFOR_Entry', 'Reference_Code', 'Isotope', 'Element', "dData", "dEnergy",
#                                 'Projectile_Z', 'Projectile_A', 'Projectile_N']
#                 cat_cols = ["Target_Metastable_State", "MT", "Product_Metastable_State", "Center_of_Mass_Flag", "I78", "Element_Flag", "O"]

#             df.drop(columns=columns_drop, inplace=True)
#             # We need to keep track of columns to normalize excluding categorical data.
#             norm_columns = len(df.columns) - len(cat_cols) - 1
#             df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)
#             logging.info("Splitting dataset into training and testing...")
#             x_train, x_test, y_train, y_test = train_test_split(df.drop(["Data"], axis=1), df["Data"], test_size=frac)
#             logging.info("Normalizing dataset...")
#             to_scale = list(x_train.columns)[:norm_columns]
#             to_scale.remove("Energy")

#             scaler = nuc_proc.normalize_features(x_train, to_scale, scaling_type, scaler_dir)

#             if scaler_dir is not None:
#                 logging.info("Using previously saved scaler.")
#                 scaler = load(open(scaler_dir, 'rb'))
#             else:
#                 logging.info("Fitting new scaler.")
#                 if scaling_type == "pt":
#                     scaler = preprocessing.PowerTransformer().fit(x_train[to_scale])
#                 elif scaling_type == "std":
#                     scaler = preprocessing.StandardScaler().fit(x_train[to_scale])
#                 elif scaling_type == "minmax":
#                     scaler = preprocessing.MinMaxScaler().fit(x_train[to_scale])

#             x_train[to_scale] = scaler.transform(x_train[to_scale])
#             x_test[to_scale] = scaler.transform(x_test[to_scale])
#             return df, x_train, x_test, y_train, y_test, to_scale, scaler
#         else:
#             logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
#             return df
#     else:
#         return logging.error("CSV file does not exists. Check given path: {}".format(datapath))