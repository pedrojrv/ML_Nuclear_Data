import os
from pathlib import Path
from joblib import dump, load
import logging
import glob
import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import nucml.config as config

logging.basicConfig(level=logging.INFO)

import nucml.general_utilities as gen_utils 
import nucml.processing as nuc_proc 

ame_dir_path = config.ame_dir_path
evaluations_path = config.evaluations_path


exfor_elements = gen_utils.load_obj(os.path.join(os.path.dirname(__file__), 'objects/exfor_elements_list.pkl'))
# element_info = gen_utils.load_obj(os.path.join(os.path.dirname(__file__), 'objects/element_basic_dict.pkl'))
dtype_exfor = gen_utils.load_obj(os.path.join(os.path.dirname(__file__), 'objects/EXFOR_AME_dtypes.pkl'))


###############################################################################
####################### ATOMIC MASS EVALUATION ################################
###############################################################################
def load_ame(natural=False, imputed_nan=False, file="merged"):    
    """Loads the Atomic Mass Evaluation 2016 data generated by NucML using the parsing utilities.

    Args:
        natural (bool): if True, the AME data will be loaded along with rows representing natural data. Only
            applicable when file='merged'.
        nan (bool): If True, the original AME data will be loaded. If False, then the linearly imputed data 
            is loaded. Only applicable when file='merged'
        file (str): Dataset to extract. Options include 'merged', 'mass16', 'rct1', and 'rct2'.
    Returns:
        ame (DataFrame): a pandas dataframe cantaining the queried AME data.

    """
    directory = ame_dir_path
    if file.lower() == "merged":
        if natural:
            if imputed_nan:
                ame_file_path = os.path.join(directory, "AME_Natural_Properties_no_NaN.csv")
            else:
                ame_file_path = os.path.join(directory, "AME_Natural_Properties_w_NaN.csv")
        else:
            if imputed_nan:
                ame_file_path = os.path.join(directory, "AME_all_merged_no_NaN.csv")
            else:
                ame_file_path = os.path.join(directory, "AME_all_merged.csv")

        logging.info("AME: Reading and loading Atomic Mass Evaluation files from: \n {}".format(ame_file_path))
        ame = pd.read_csv(ame_file_path)
        ame[["N", "Z", "A"]] = ame[["N", "Z", "A"]].astype(int)
    elif file.lower() in ["mass16", "rct1", "rct2"]:
        ame_file_path = os.path.join(ame_dir_path, "AME_{}.csv".format(file))
        logging.info("AME: Reading and loading the Atomic Mass Evaluation file from: \n {}".format(ame_file_path))
        ame = pd.read_csv(ame_file_path)
    return ame


###############################################################################
####################### EVALUATED LIBRARIES ###################################
###############################################################################
def load_evaluation(ELAAA, MT, mode="neutrons", library="endfb8.0", mev_to_ev=True, mb_to_b=True, log=True, drop_u=True):
    """Reads an evaluation file for a specific element and reaction channel. It is important
    to inspect the returned data since it queries an external database which extracted data from ENDF 
    using a particular script. It has been found that some particular reactions are not included.

    Args:
        ELAAA (str): element to query. The string must start with the element followed by the mass number (i.e. U233, Cl35) 
        MT (int): reaction channel to query. Must be an integer (i.e. 1, 2, 3)
        mode (str): projectile of the reaction of interest. Only "neutrons" and "protons" is allowed for now.
        library (str): evaluation library to query. Allowed options include endfb8.0, jendl4.0, jeff3.3, and tendl.2019.
        mev_to_ev (bool): if True, it converts the energy from MeV to eV.
        mb_to_b (bool): if True, it converts the cross sections from millibarns to barns.
        log (bool): if True, it applies the log10 to both the Energy and the Cross Section.
        drop_u (bool): sometimes, evaluation files contain uncertainty values. If True, these features are removed.
    Returns:
        evaluation (DataFrame): pandas DataFrame containing the ENDF datapoints.
    """
    MT = gen_utils.parse_mt(MT)
    ELAAA = gen_utils.parse_elaaa(ELAAA, parse_for='endf')

    if mode == "protons":
        projectile = 'p'
    elif mode == "neutrons":
        projectile = 'n'

    path = os.path.join(evaluations_path, '{}/{}/{}/tables/xs/{}-{}-{}.{}'.format(mode, ELAAA, library, projectile, ELAAA, MT, library))    

    file = Path(path)
    if file.is_file():
        logging.info("EVALUATION: Extracting data from {}".format(path))
        evaluation = pd.read_csv(path, skiprows=5, header=None, names=["Energy", "Data", "dDataLow", "dDataUpp"], delim_whitespace=True)
        if mev_to_ev:
            logging.info("EVALUATION: Converting MeV to eV...")
            evaluation["Energy"] = evaluation["Energy"]*1E6
        if mb_to_b:
            logging.info("EVALUATION: Converting mb to b...")
            evaluation["Data"] = evaluation["Data"]*0.001
        if log:
            evaluation["Energy"] = np.log10(evaluation["Energy"])
            evaluation["Data"] = np.log10(evaluation["Data"])
            evaluation["dDataLow"] = np.log10(evaluation["dDataLow"])
            evaluation["dDataUpp"] = np.log10(evaluation["dDataUpp"])
        if drop_u:
            if "dData" in list(evaluation.columns):
                evaluation = evaluation.drop(columns=["dDataLow"])
            if "dData2" in list(evaluation.columns):
                evaluation = evaluation.drop(columns=["dDataUpp"])
            if "dDataLow" in list(evaluation.columns):
                evaluation = evaluation.drop(columns=["dDataLow"])
            if "dDataUpp" in list(evaluation.columns):
                evaluation = evaluation.drop(columns=["dDataUpp"])
        logging.info("EVALUATION: Finished. ENDF data contains {} datapoints.".format(evaluation.shape[0]))
        return evaluation
    else:
        raise FileNotFoundError('Evaluation file does not exists at {}'.format(path))



        
###############################################################################
####################### ENSDF LIBRARIES #######################################
###############################################################################
def load_ensdf(cutoff=False, log_sqrt=False, log=False, append_ame=False, basic=-1, num=False, frac=0.3, scaling_type="pt", scaler_dir=None):
    """Loads the Evalauted Nuclear Structure Data File generated using NucML. This allows the user to load
    the raw file or preprocessed the dataset for ML applications. See options below.

    The basic feature allows you to load only some basic features if needed. The AME dataset contains
    many features including q-reactions and separation energies. Some of these may not be needed. The
    basic argument allows to quickly remove extra features. 
        basic = 0: "Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", "Atomic_Mass_Micro"
        basic = 1: "Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", 
            "Atomic_Mass_Micro", 'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'S(2n)', 'S(n)', 'S(p)'
        Any other number will default to loading the entire set of features.

    Args:
        cutoff (bool, optional): If True, the RIPL cutoff ENSDF file is loaded. Defaults to False.
        log (bool, optional): If True, the log10 is applied to the Level Number feature. It also applies 
            the square root to the Level Energy feature. Defaults to False.
        append_ame (bool, optional): if True, it appends the AME database. Defaults to False.
        basic (int, optional): This allows to retrieve only basic features. 
            Only meaningful when append_ame is True. Defaults to -1.
        num (bool, optional): [description]. Defaults to False.
        frac (float, optional): [description]. Defaults to 0.3.
        scaling_type (str, optional): [description]. Defaults to "pt".
        scaler_dir ([type], optional): [description]. Defaults to None.

    Returns:
        DataFrame: if num=True, the function returns 6 variables. 
    """    
    if cutoff:
        datapath = "../../ENSDF/CSV_Files/ensdf_cutoff.csv"
    else:
        datapath = "../../ENSDF/CSV_Files/ensdf.csv"
    if os.path.exists(datapath):
        logging.info("Reading data from {}".format(datapath))
        df = pd.read_csv(datapath)
        df = df[~(df["Parity"] == 1620)] # THIS ELIMINATES A STRANGE DATAPOINT
        df["Level_Number"] = df["Level_Number"].astype(int)
        df[["Spin", "Parity", "Target_Element_w_A"]] = df[["Spin", "Parity", "Target_Element_w_A"]].astype('category')
        if log_sqrt:
            df["Level_Energy"] = np.sqrt(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if log:
            logging.info("Dropping Ground State...")
            df = df[(df["Level_Energy"] != 0)]
            df["Level_Energy"] = np.log10(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if append_ame:
            ame = load_ame(natural=False, nan=False)
            ame = ame.rename(columns={"Element_w_A": "Target_Element_w_A", "Z": "Protons", "N": "Neutrons", "A": "Mass_Number"})
            df = pd.merge(df, ame, on='Target_Element_w_A')
        if basic == 0:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", "Atomic_Mass_Micro"]
            df = df[basic_cols]
        elif basic == 1:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Spin", "Parity", "Atomic_Mass_Micro",
                        'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'S(2n)', 'S(n)', 'S(p)']
            df = df[basic_cols] 
        if num:
            logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")
            if basic == 0 or basic == 1:
                cat_cols = ["Parity"]
            else:
                columns_drop = ["Target_Element_w_A", "EL", "O"]
                cat_cols = ["Parity"]
                df = df.drop(columns=columns_drop)
            # We need to keep track of columns to normalize excluding categorical data.
            norm_columns = len(df.columns) - len(cat_cols) - 1
            df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)
            df = df.fillna(value=0)
            logging.info("Splitting dataset into training and testing...")
            x_train, x_test, y_train, y_test = train_test_split(df.drop(["Level_Energy"], axis=1), df["Level_Energy"], test_size=frac)
            logging.info("Normalizing dataset...")
            to_scale = list(x_train.columns)[:norm_columns]
            if log_sqrt or log:
                to_scale.remove("Level_Number")
            if scaler_dir is not None:
                logging.info("Using previously saved scaler.")
                scaler = load(open(scaler_dir, 'rb'))
            else:
                logging.info("Fitting new scaler.")
                if scaling_type == "pt":
                    scaler = preprocessing.PowerTransformer().fit(x_train[to_scale])
                elif scaling_type == "std":
                    scaler = preprocessing.StandardScaler().fit(x_train[to_scale])
                elif scaling_type == "minmax":
                    scaler = preprocessing.MinMaxScaler().fit(x_train[to_scale])
            x_train[to_scale] = scaler.transform(x_train[to_scale])
            x_test[to_scale] = scaler.transform(x_test[to_scale])
            logging.info("Finished. Resulting dataset has shape {}, Training and Testing dataset shapes are {} and {} respesctively.".format(df.shape, x_train.shape, x_test.shape))
            return df, x_train, x_test, y_train, y_test, to_scale, scaler
        else:
            logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
            return df
    else:
        return logging.error("CSV file does not exists. Check path.")

def load_ripl_parameters():
    """Loads the RIPL level cut-off parameters file.

    Returns:
        DataFrame: pandas DataFrame 
    """    
    ripl_params = pd.read_csv("../ENSDF/CSV_Files/ripl_cut_off_energies.csv")
    return ripl_params

def load_ensdf_ml(log_sqrt=False, log=False, append_ame=False, basic=-1, num=False, frac=0.3, scaling_type="std", scaler_dir=None):
    path = '../../ENSDF/CSV_Files/Level_Density' # use your path
    all_files = glob.glob(path + "/*.csv")

    if len(all_files) != 0:

        li = []

        for filename in all_files:
            df = pd.read_csv(filename, index_col=None, header=0)
            li.append(df)

        df = pd.concat(li, axis=0, ignore_index=True)

        df["Level_Number"] = df["Level_Number"].astype(int)
        df[["Target_Element_w_A"]] = df[["Target_Element_w_A"]].astype('category')
        if log_sqrt:
            df["Level_Energy"] = np.sqrt(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if log:
            logging.info("Dropping Ground State...")
            df = df[(df["Level_Energy"] != 0)]
            df["Level_Energy"] = np.log10(df["Level_Energy"])
            df["Level_Number"] = np.log10(df["Level_Number"])
        if append_ame:
            ame = load_ame(natural=False, nan=False)
            ame = ame.rename(columns={"Element_w_A": "Target_Element_w_A", "Z": "Protons", "N": "Neutrons", "A": "Mass_Number"})
            df = pd.merge(df, ame, on='Target_Element_w_A')
        if basic == 0:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Atomic_Mass_Micro"]
            df = df[basic_cols]
        elif basic == 1:
            basic_cols = ["Level_Number", "Level_Energy", "Protons", "Neutrons", "Mass_Number", "Atomic_Mass_Micro",
                        'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'S(2n)', 'S(n)', 'S(p)']
            df = df[basic_cols] 
        if num:
            logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")

            # We need to keep track of columns to normalize excluding categorical data.
            df = df.fillna(value=0)
            logging.info("Splitting dataset into training and testing...")
            x_train, x_test, y_train, y_test = train_test_split(df.drop(["Level_Energy"], axis=1), df["Level_Energy"], test_size=frac)
            logging.info("Normalizing dataset...")
            to_scale = list(x_train.columns)
            if log_sqrt or log:
                to_scale.remove("Level_Number")
            if scaler_dir is not None:
                logging.info("Using previously saved scaler.")
                scaler = load(open(scaler_dir, 'rb'))
            else:
                logging.info("Fitting new scaler.")
                if scaling_type == "pt":
                    scaler = preprocessing.PowerTransformer().fit(x_train[to_scale])
                elif scaling_type == "std":
                    scaler = preprocessing.StandardScaler().fit(x_train[to_scale])
                elif scaling_type == "minmax":
                    scaler = preprocessing.MinMaxScaler().fit(x_train[to_scale])
            x_train[to_scale] = scaler.transform(x_train[to_scale])
            x_test[to_scale] = scaler.transform(x_test[to_scale])
            logging.info("Finished. Resulting dataset has shape {}, Training and Testing dataset shapes are {} and {} respesctively.".format(df.shape, x_train.shape, x_test.shape))
            return df, x_train, x_test, y_train, y_test, to_scale, scaler
        else:
            logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
            return df
    else:
        return logging.error("CSV file does not exists. Check path.")


###############################################################################
####################### EXFOR DATABASE ########################################
###############################################################################
supported_modes = ["neutrons", "protons", "alphas", "deuterons", "gammas", "helions", "all"]
supported_mt_coding = ["one_hot", "particle_coded"]
def load_exfor(log=False, low_en=False, basic=-1, num=False, frac=0.1, mode="neutrons", scaling_type="standard", 
    scaler_dir=None, filters=False, max_en=2.0E7, mt_coding="one_hot", scale_energy=False):

    if mode not in supported_modes:
        return logging.error("Specified MODE not supported. Supporte modes include: {}".format(' '.join([str(v) for v in supported_modes])))
    if mt_coding not in supported_mt_coding:
        return logging.error("Specified mt_coding not supported. Supported codings include: {}".format(' '.join([str(v) for v in supported_mt_coding])))

    logging.info(" MODE: {}".format(mode))
    logging.info(" LOW ENERGY: {}".format(low_en))
    logging.info(" LOG: {}".format(log))
    logging.info(" BASIC: {}".format(basic))
    logging.info(" SCALER: {}".format(scaling_type.upper()))

    if mode == "all":
        neutrons_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "neutrons" + '\\EXFOR_' + "neutrons" + '_MF3_AME_no_RawNaN.csv'
        protons_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "protons" + '\\EXFOR_' + "protons" + '_MF3_AME_no_RawNaN.csv'
        alphas_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "alphas" + '\\EXFOR_' + "alphas" + '_MF3_AME_no_RawNaN.csv'
        deuterons_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "deuterons" + '\\EXFOR_' + "deuterons" + '_MF3_AME_no_RawNaN.csv'
        gammas_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "gammas" + '\\EXFOR_' + "gammas" + '_MF3_AME_no_RawNaN.csv'
        helions_datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + "helions" + '\\EXFOR_' + "helions" + '_MF3_AME_no_RawNaN.csv'
        all_datapaths = [neutrons_datapath, protons_datapath, alphas_datapath, deuterons_datapath, gammas_datapath, helions_datapath]
        if gen_utils.check_if_files_exist(all_datapaths):
            df = pd.read_csv(neutrons_datapath, dtype=dtype_exfor).dropna()
            protons = pd.read_csv(protons_datapath, dtype=dtype_exfor).dropna()
            alphas = pd.read_csv(alphas_datapath, dtype=dtype_exfor).dropna()
            deuterons = pd.read_csv(deuterons_datapath, dtype=dtype_exfor).dropna()
            gammas = pd.read_csv(gammas_datapath, dtype=dtype_exfor).dropna()
            helions = pd.read_csv(helions_datapath, dtype=dtype_exfor).dropna()
            df = df.append([protons, alphas, deuterons, gammas, helions])
        else:
            return logging.error("One ore more files are missing. Check directories.")
    else:
        datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + mode + '\\EXFOR_' + mode + '_MF3_AME_no_RawNaN.csv'
        if os.path.exists(datapath):
            logging.info("Reading data from {}".format(datapath))
            df = pd.read_csv(datapath, dtype=dtype_exfor).dropna()
        else:
            return logging.error("CSV file does not exists. Check given path: {}".format(datapath))
        
    if filters:
        df = df[~((df.Reaction_Notation.str.contains("WTR")) | (df.Title.str.contains("DERIV")) | (df.Energy == 0) | (df.Data == 0))]
        df = df[(df.MT != "203") & (df.MT != "1003") & (df.MT != "1108") & (df.MT != "2103")]
    if low_en:
        df = df[df.Energy < max_en]
    if log:
        if (df[df.Energy == 0].shape[0] != 0) or (df[df.Data == 0].shape[0] != 0):
            logging.error("Cannot take log. Either Energy or Data contain zeros. Ignoring log.")
        else:
            df["Energy"] = np.log10(df["Energy"])
            df["Data"] = np.log10(df["Data"])

    magic_numbers = [2, 8, 20, 28, 40, 50, 82, 126, 184]
    df["N_valence"] = df.N.apply(lambda neutrons: abs(neutrons - min(magic_numbers, key=lambda x:abs(x-neutrons))))
    df["Z_valence"] = df.Z.apply(lambda protons: abs(protons - min(magic_numbers, key=lambda x:abs(x-protons))))
    df["P_factor"] = (df["N_valence"] * df["Z_valence"]) / (df["N_valence"] + df["Z_valence"])
    df.P_factor = df.P_factor.fillna(0)
    df["N_tag"] = df.N_valence.apply(lambda neutrons: "even" if neutrons % 2 == 0 else "odd")
    df["Z_tag"] = df.Z_valence.apply(lambda protons: "even" if protons % 2 == 0 else "odd")
    df["NZ_tag"] = df["N_tag"] + "_" + df["Z_tag"]

    if basic == 0:
        basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag", "N", "A", "Element_Flag"]
        df = df[basic_cols]
    elif basic == 1:
        basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag",
                    "N", "A", "Element_Flag", 'Nucleus_Radius', 'Neutron_Nucleus_Radius_Ratio', 
                    'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'Atomic_Mass_Micro', 'S(2n)', 
                    'S(n)', 'S(p)', 'S(2p)', "N_valence", "Z_valence", "P_factor", "N_tag", "Z_tag", 
                    "NZ_tag"]
        df = df[basic_cols]  
    elif basic == 2:
        basic_cols = df.columns
        basic_cols = [x for x in basic_cols if not x.startswith("d")]
        df = df[basic_cols]  
    elif basic == 3:
        basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag", "N", "A", "Element_Flag", 
                    'Nucleus_Radius', 'Neutron_Nucleus_Radius_Ratio', 'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 
                    'Atomic_Mass_Micro', 'S(n)', 'S(p)']
        df = df[basic_cols]

    logging.info("Data read into dataframe with shape: {}".format(df.shape))
    if num:
        logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")
        if basic == 0:
            columns_drop = ["dData", "dEnergy"]
            cat_cols = ["MT", "Center_of_Mass_Flag", "Element_Flag"]
        elif basic == 1:
            columns_drop = ["dData", "dEnergy"]
            cat_cols = ["MT", "Center_of_Mass_Flag", "Element_Flag", "N_tag", "Z_tag", "NZ_tag"]
        elif basic == 2:
            columns_drop = ["Projectile", "EXFOR_Status", "Short_Reference", 'EXFOR_Accession_Number', 'EXFOR_SubAccession_Number', 
                            'EXFOR_Pointer', "Reaction_Notation", "Title", "Year", "Author", "Institute", "Date", "Reference",
                            'Dataset_Number', 'EXFOR_Entry', 'Reference_Code', 'Isotope', 'Element',
                            'Projectile_Z', 'Projectile_A', 'Projectile_N', "ELV/HL", "Target_Metastable_State", 
                            "Product_Metastable_State", "I78", "O"]
            cat_cols = ["MT", "Center_of_Mass_Flag", "Element_Flag", "N_tag", "Z_tag", "NZ_tag"]
        elif basic == 3:
            columns_drop = ["dData", "dEnergy"]
            cat_cols = ["MT", "Center_of_Mass_Flag", "Element_Flag"]
        else:
            columns_drop = ["Projectile", "EXFOR_Status", "Short_Reference", 'EXFOR_Accession_Number', 'EXFOR_SubAccession_Number', 
                            'EXFOR_Pointer', "Reaction_Notation", "Title", "Year", "Author", "Institute", "Date", "Reference",
                            'Dataset_Number', 'EXFOR_Entry', 'Reference_Code', 'Isotope', 'Element', "dData", "dEnergy",
                            'Projectile_Z', 'Projectile_A', 'Projectile_N']
            cat_cols = ["Target_Metastable_State", "MT", "Product_Metastable_State", "Center_of_Mass_Flag", "I78", "Element_Flag", "O", "N_tag", "Z_tag", "NZ_tag"]
        
            
        df.drop(columns=columns_drop, inplace=True)
        
        if mt_coding == "particle_coded":
            cat_cols.remove("MT")
            mt_codes_df = pd.read_csv('C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\mt_codes.csv').drop(columns=["MT_Tag", "MT_Reaction_Notation"])
            mt_codes_df["MT"] = mt_codes_df["MT"].astype(str)
            # We need to keep track of columns to normalize excluding categorical data.
            norm_columns = len(df.columns) - len(cat_cols) - 2
            df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)
            df = pd.merge(df, mt_codes_df, on='MT').drop(columns=["MT"])
        elif mt_coding == "one_hot":
            # We need to keep track of columns to normalize excluding categorical data.
            norm_columns = len(df.columns) - len(cat_cols) - 1
            df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)


        logging.info("Splitting dataset into training and testing...")
        x_train, x_test, y_train, y_test = train_test_split(df.drop(["Data"], axis=1), df["Data"], test_size=frac)
        
        logging.info("Normalizing dataset...")
        to_scale = list(x_train.columns)[:norm_columns]
        if not scale_energy:
            to_scale.remove("Energy")
        scaler = nuc_proc.normalize_features(x_train, to_scale, scaling_type, scaler_dir)
        x_train[to_scale] = scaler.transform(x_train[to_scale])
        x_test[to_scale] = scaler.transform(x_test[to_scale])
        return df, x_train, x_test, y_train, y_test, to_scale, scaler
    else:
        logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
        return df




# IMPLEMENT ADDITION OF STABLE STATES
# columns_ensdf = ["Element_w_A", "N1", "Elv[MeV]", "spin", "parity", "state_half_life", "Ng", "J", "unc", "spins", "nd", 
#                  "m", "percent", "mode", "other", "other1", "other2", "other3", "other4"]
# ensdf_final = pd.read_csv(resulting_files_dir + "ensdf_stable_state_formatted.csv", names=columns_ensdf, sep=";")
# ensdf_final["spin"] = ensdf_final["spin"].replace(to_replace=-1.0, value=3.5) 
# ensdf_final["parity"] = ensdf_final["parity"].replace(to_replace=0, value=1.0)
# ensdf_final["Element_w_A"] = ensdf_final["Element_w_A"].apply(lambda x: x.strip())
# ensdf_final = ensdf_final[["Element_w_A", "spin", "parity"]]
# df2 = pd.merge(df, ensdf_final, on='Element_w_A')

# ADD RIPLE LEVEL PARAMETERES LOADING
# cut_off_cols = ["Z", "A", "Element", "Temperature_MeV", "Temperature_U", "Black_Shift", 
#                 "Black_Shift_U", "N_Lev_ENSDF", "N_Max_Lev_Complete", "Min_Lev_Complete", 
#                 "Num_Lev_Unique_Spin", "E_Max_N_Max", "E_Num_Lev_U_Spin", "Other", "Other2", 
#                 "Flag", "Nox", "Other3", "Other4", "Spin_Cutoff"]
# cut_off = pd.read_csv("./ENSDF/Resulting_Files/cut_off_ensdf_energies.csv", names=cut_off_cols, sep=";")

# cut_off.tail()

# df.dtypes.apply(lambda x: x.name).to_dict()


# # ADD CAPABILITY TO ADD ENSDF AND AME SEPARATEDLEY
# supported_modes = ["neutrons", "protons", "alphas", "deuterons", "gammas", "helions", "all"]
# def load_exfor(log=False, low_en=True, basic=-1, num=False, frac=0.1, mode="neutrons", scaling_type="pt", scaler_dir=None, filters=True):
#     """[summary]

#     Args:
#         log (bool, optional): [description]. Defaults to False.
#         low_en (bool, optional): [description]. Defaults to True.
#         basic (int, optional): [description]. Defaults to -1.
#         num (bool, optional): [description]. Defaults to False.
#         frac (float, optional): [description]. Defaults to 0.1.
#         mode (str, optional): [description]. Defaults to "neutrons".
#         scaling_type (str, optional): [description]. Defaults to "pt".
#         scaler_dir ([type], optional): [description]. Defaults to None.

#     Returns:
#         [type]: [description]
#     """
    
#     ######## SETTING UP VARIABLES FOR DATA EXTRACTION ##########
#     if mode not in supported_modes:
#         return logging.error("Specified MODE not supported. Supporte modes include: {}".format(' '.join([str(v) for v in supported_modes])))

#     logging.info(" MODE: {}".format(mode))
#     logging.info(" LOW ENERGY: {}".format(low_en))
#     logging.info(" LOG: {}".format(log))
#     logging.info(" BASIC: {}".format(basic))
#     logging.info(" SCALER: {}".format(scaling_type.upper()))

#     datapath = 'C:\\Users\\Pedro\\Desktop\\ML_Nuclear_Data\\EXFOR\\CSV_Files\\EXFOR_' + mode + '\\EXFOR_' + mode + '_MF3_AME_no_RawNaN.csv'
#     if os.path.exists(datapath):
#         logging.info("Reading data from {}".format(datapath))
#         df = pd.read_csv(datapath, dtype=dtype_exfor).dropna()

#         if filters:
#             df = df[~((df.Reaction_Notation.str.contains("WTR")) | (df.Title.str.contains("DERIV")) | (df.Energy == 0) | (df.Data == 0))]

#         if low_en:
#             df = df[df.Energy < 2.0E7]

#         if log:
#             if (df[df.Energy == 0].shape[0] != 0) or (df[df.Data == 0].shape[0] != 0):
#                 logging.error("Cannot take log. Either Energy or Data contain zeros. Ignoring log.")
#             else:
#                 df["Energy"] = np.log10(df["Energy"])
#                 df["Data"] = np.log10(df["Data"])

#         if basic == 0:
#             basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag", "N", "A", "Element_Flag"]
#             df = df[basic_cols]
#         elif basic == 1:
#             # 'S(2p)' THIS IS AN ISSUE WITH YEO TRANSFORMER FOR SOME REASON
#             basic_cols = ["Energy", "dEnergy", "Data", "dData", "MT", "Z", "Center_of_Mass_Flag",
#                         "N", "A", "Element_Flag", 'Nucleus_Radius', 'Neutron_Nucleus_Radius_Ratio', 
#                         'Mass_Excess', 'Binding_Energy', 'B_Decay_Energy', 'Atomic_Mass_Micro', 'S(2n)', 
#                         'S(n)', 'S(p)']
#             df = df[basic_cols]  

#         logging.info("Data read into dataframe with shape: {}".format(df.shape))
#         if num:
#             logging.info("Dropping unnecessary features and one-hot encoding categorical columns...")
#             if basic == 0 or basic == 1:
#                 columns_drop = ["dData", "dEnergy"]
#                 cat_cols = ["MT", "Center_of_Mass_Flag", "Element_Flag"]
#             else:
#                 columns_drop = ["Projectile", "EXFOR_Status", "Short_Reference", 'EXFOR_Accession_Number', 'EXFOR_SubAccession_Number', 
#                                 'EXFOR_Pointer', "Reaction_Notation", "Title", "Year", "Author", "Institute", "Date", "Reference",
#                                 'Dataset_Number', 'EXFOR_Entry', 'Reference_Code', 'Isotope', 'Element', "dData", "dEnergy",
#                                 'Projectile_Z', 'Projectile_A', 'Projectile_N']
#                 cat_cols = ["Target_Metastable_State", "MT", "Product_Metastable_State", "Center_of_Mass_Flag", "I78", "Element_Flag", "O"]

#             df.drop(columns=columns_drop, inplace=True)
#             # We need to keep track of columns to normalize excluding categorical data.
#             norm_columns = len(df.columns) - len(cat_cols) - 1
#             df = pd.concat([df, pd.get_dummies(df[cat_cols])], axis=1).drop(columns=cat_cols)
#             logging.info("Splitting dataset into training and testing...")
#             x_train, x_test, y_train, y_test = train_test_split(df.drop(["Data"], axis=1), df["Data"], test_size=frac)
#             logging.info("Normalizing dataset...")
#             to_scale = list(x_train.columns)[:norm_columns]
#             to_scale.remove("Energy")

#             scaler = nuc_proc.normalize_features(x_train, to_scale, scaling_type, scaler_dir)

#             if scaler_dir is not None:
#                 logging.info("Using previously saved scaler.")
#                 scaler = load(open(scaler_dir, 'rb'))
#             else:
#                 logging.info("Fitting new scaler.")
#                 if scaling_type == "pt":
#                     scaler = preprocessing.PowerTransformer().fit(x_train[to_scale])
#                 elif scaling_type == "std":
#                     scaler = preprocessing.StandardScaler().fit(x_train[to_scale])
#                 elif scaling_type == "minmax":
#                     scaler = preprocessing.MinMaxScaler().fit(x_train[to_scale])

#             x_train[to_scale] = scaler.transform(x_train[to_scale])
#             x_test[to_scale] = scaler.transform(x_test[to_scale])
#             return df, x_train, x_test, y_train, y_test, to_scale, scaler
#         else:
#             logging.info("Finished. Resulting dataset has shape {}".format(df.shape))
#             return df
#     else:
#         return logging.error("CSV file does not exists. Check given path: {}".format(datapath))